{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcartopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mccrs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = '/Users/jack/Library/CloudStorage/GoogleDrive-limjackailjk@gmail.com/My Drive/UCSD/DSC/DSC180/ClimateBench - Plus/ClimateBench-Plus/DKL Gaussian Process/data/processed_data/'\n",
    "datapath = 'G://My Drive//UCSD//DSC//DSC180//ClimateBench - Plus//ClimateBench-Plus//DKL Gaussian Process//data//processed_data//'\n",
    "simulations = ['ssp126', 'ssp370', 'ssp585', 'hist-GHG', 'hist-aer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp126\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp370\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp585\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-GHG\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-aer\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i, simu in enumerate(simulations):\n",
    "    input_name = 'inputs_' + simu + '.nc'\n",
    "    output_name = 'outputs_' + simu + '.nc'\n",
    "    # Just load hist data in these cases 'hist-GHG' and 'hist-aer'\n",
    "    if 'hist' in simu:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_dataset(datapath + input_name)\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.open_dataset(datapath + output_name).mean(dim='member')\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400, \"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "    \n",
    "    # Concatenate with historical data in the case of scenario 'ssp126', 'ssp370' and 'ssp585'\n",
    "    else:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_mfdataset([datapath + 'inputs_historical.nc', datapath + input_name]).compute()\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.concat([xr.open_dataset(datapath + 'outputs_historical.nc').mean(dim='member'),\n",
    "                               xr.open_dataset(datapath + output_name).mean(dim='member')],\n",
    "                               dim='time').compute()\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400,\"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "\n",
    "    print(input_xr.dims, simu)\n",
    "\n",
    "    # Append to list \n",
    "    X_train.append(input_xr)\n",
    "    Y_train.append(output_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074.172303244536, 1755.690699230666)\n",
      "(0.1927369743762821, 0.18457590641432994)\n",
      "(2.5623359997066755e-12, 2.250114566783271e-11)\n",
      "(1.4947905009818064e-13, 1.0313342554838387e-12)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean/std of each variable for the whole dataset\n",
    "meanstd_inputs = {}\n",
    "len_historical = 165\n",
    "\n",
    "for var in ['CO2', 'CH4', 'SO2', 'BC']:\n",
    "    # To not take the historical data into account several time we have to slice the scenario datasets\n",
    "    # and only keep the historical data once (in the first ssp index 0 in the simus list)\n",
    "    array = np.concatenate([X_train[i][var].data for i in [0, 3, 4]] + \n",
    "                           [X_train[i][var].sel(time=slice(len_historical, None)).data for i in range(1, 3)])\n",
    "    print((array.mean(), array.std()))\n",
    "    meanstd_inputs[var] = (array.mean(), array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input data \n",
    "X_train_norm = [] \n",
    "for i, train_xr in enumerate(X_train): \n",
    "    for var in ['CO2', 'CH4', 'SO2', 'BC']: \n",
    "        var_dims = train_xr[var].dims\n",
    "        train_xr=train_xr.assign({var: (var_dims, normalize(train_xr[var].data, var, meanstd_inputs))}) \n",
    "    X_train_norm.append(train_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 96, 144, 4)\n",
      "(726, 96, 144, 1)\n"
     ]
    }
   ],
   "source": [
    "var_to_predict =  'tas'\n",
    "# skip_historical set to (i < 2) because of the order of the scenario and historical runs in the X_train and Y_train lists.\n",
    "# In details: ssp126 0, ssp370 1 = skip historical part of the data, ssp585 2, hist-GHG 3 and hist-aer 4 = keep the whole sequence\n",
    "X_train_all = np.concatenate([input_for_training(X_train_norm[i], skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis = 0)\n",
    "Y_train_all = np.concatenate([output_for_training(Y_train[i], var_to_predict, skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis=0)\n",
    "# add a dimension to the output data\n",
    "Y_train_all = Y_train_all[..., np.newaxis]\n",
    "print(X_train_all.shape)\n",
    "print(Y_train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Neural Process Arch\n",
    "- Encoder --> Takes context points x_i, y_i to r_i (deterministic path) and s_i (latent path)\n",
    "- **r_c, s_c** from context points representations aggregated by mean\n",
    "- Decoder --> Takes r_c, s_c, target points x_i to predict y_i\n",
    "\n",
    "npf --> https://yanndubs.github.io/Neural-Process-Family/text/Intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"/Users/jack/Library/CloudStorage/GoogleDrive-limjackailjk@gmail.com/My Drive/Research/ClimateBench/Spring TRELS - Neural Process/Neural Process/ClimateBenchNPs/Neural_Process_Family\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'npf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from Neural_Process_Family.npf.neuralproc import ConvCNP, GridConvCNP\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnpf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchitectures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CNN, MLP, ResConvBlock, SetConv, discard_ith_arg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnpf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CircularPad2d, make_abs_conv, make_padded_conv\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count_parameters\n",
      "File \u001b[1;32mg:\\My Drive\\Research\\ClimateBench\\Spring TRELS - Neural Process\\Neural Process\\ClimateBenchNPs\\Neural_Process_Family\\npf\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneuralproc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mg:\\My Drive\\Research\\ClimateBench\\Spring TRELS - Neural Process\\Neural Process\\ClimateBenchNPs\\Neural_Process_Family\\npf\\losses.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightTailPareto\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     LightTailPareto,\n\u001b[0;32m     10\u001b[0m     dist_to_device,\n\u001b[0;32m     11\u001b[0m     logcumsumexp,\n\u001b[0;32m     12\u001b[0m     sum_from_nth_dim,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kl_divergence\n",
      "File \u001b[1;32mg:\\My Drive\\Research\\ClimateBench\\Spring TRELS - Neural Process\\Neural Process\\ClimateBenchNPs\\Neural_Process_Family\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mg:\\My Drive\\Research\\ClimateBench\\Spring TRELS - Neural Process\\Neural Process\\ClimateBenchNPs\\Neural_Process_Family\\utils\\visualize\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize_1d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize_imgs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mg:\\My Drive\\Research\\ClimateBench\\Spring TRELS - Neural Process\\Neural Process\\ClimateBenchNPs\\Neural_Process_Family\\utils\\visualize\\visualize_1d.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnpf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneuralproc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentNeuralProcessFamily\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNeural_Process_Family\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rescale_range\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'npf'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# from Neural_Process_Family.npf.neuralproc import ConvCNP, GridConvCNP\n",
    "from Neural_Process_Family.npf.architectures import CNN, MLP, ResConvBlock, SetConv, discard_ith_arg\n",
    "from Neural_Process_Family.npf.utils.helpers import CircularPad2d, make_abs_conv, make_padded_conv\n",
    "from Neural_Process_Family.utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_DIM = 128\n",
    "KWARGS = dict(\n",
    "    r_dim=R_DIM,\n",
    "    Decoder=discard_ith_arg(  # disregards the target features to be translation equivariant\n",
    "        partial(MLP, n_hidden_layers=4, hidden_size=R_DIM), i=0\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "CNN_KWARGS = dict(\n",
    "    ConvBlock=ResConvBlock,\n",
    "    is_chan_last=True,  # all computations are done with channel last in our code\n",
    "    n_conv_layers=2,  # layers per block\n",
    ")\n",
    "\n",
    "\n",
    "# off the grid\n",
    "model_1d = partial(\n",
    "    ConvCNP,\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    Interpolator=SetConv,\n",
    "    CNN=partial(\n",
    "        CNN,\n",
    "        Conv=torch.nn.Conv1d,\n",
    "        Normalization=torch.nn.BatchNorm1d,\n",
    "        n_blocks=5,\n",
    "        kernel_size=19,\n",
    "        **CNN_KWARGS,\n",
    "    ),\n",
    "    density_induced=64,  # density of discretization\n",
    "    **KWARGS,\n",
    ")\n",
    "\n",
    "\n",
    "# on the grid\n",
    "model_2d = partial(\n",
    "    GridConvCNP,\n",
    "    x_dim=1,  # for gridded conv it's the mask shape\n",
    "    CNN=partial(\n",
    "        CNN,\n",
    "        Conv=torch.nn.Conv2d,\n",
    "        Normalization=torch.nn.BatchNorm2d,\n",
    "        n_blocks=5,\n",
    "        kernel_size=9,\n",
    "        **CNN_KWARGS,\n",
    "    ),\n",
    "    **KWARGS,\n",
    ")\n",
    "\n",
    "# full translation equivariance\n",
    "\n",
    "Padder = CircularPad2d\n",
    "\n",
    "model_2d_extrap = partial(\n",
    "    GridConvCNP,\n",
    "    x_dim=1,  # for gridded conv it's the mask shape\n",
    "    CNN=partial(\n",
    "        CNN,\n",
    "        Normalization=partial(torch.nn.BatchNorm2d, eps=1e-2),  # was getting NaN\n",
    "        Conv=make_padded_conv(torch.nn.Conv2d, Padder),\n",
    "        n_blocks=5,\n",
    "        kernel_size=9,\n",
    "        **CNN_KWARGS,\n",
    "    ),\n",
    "    # make first layer also padded (all arguments are defaults besides `make_padded_conv` given `Padder`)\n",
    "    Conv=lambda y_dim: make_padded_conv(make_abs_conv(torch.nn.Conv2d), Padder)(\n",
    "        y_dim, y_dim, groups=y_dim, kernel_size=11, padding=11 // 2, bias=False,\n",
    "    ),\n",
    "    **KWARGS,\n",
    ")\n",
    "\n",
    "# large model\n",
    "model_2d_XL = partial(\n",
    "    GridConvCNP,\n",
    "    x_dim=1,  # for gridded conv it's the mask shape\n",
    "    CNN=partial(\n",
    "        CNN,\n",
    "        Conv=torch.nn.Conv2d,\n",
    "        Normalization=torch.nn.BatchNorm2d,\n",
    "        n_blocks=12,\n",
    "        kernel_size=9,\n",
    "        **CNN_KWARGS,\n",
    "    ),\n",
    "    **KWARGS,\n",
    ")\n",
    "\n",
    "\n",
    "n_params_1d = count_parameters(model_1d())\n",
    "n_params_2d = count_parameters(model_2d(y_dim=3))\n",
    "n_params_2d_XL = count_parameters(model_2d_XL(y_dim=3))\n",
    "\n",
    "print(f\"Number Parameters (1D): {n_params_1d:,d}\")\n",
    "print(f\"Number Parameters (2D): {n_params_2d:,d}\")\n",
    "print(f\"Number Parameters (2D XL): {n_params_2d_XL:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClimateBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
