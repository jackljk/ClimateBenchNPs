{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/jack/Library/CloudStorage/GoogleDrive-limjackailjk@gmail.com/My Drive/UCSD/DSC/DSC180/ClimateBench - Plus/ClimateBench-Plus/DKL Gaussian Process/data/processed_data/'\n",
    "# datapath = 'G://My Drive//UCSD//DSC//DSC180//ClimateBench - Plus//ClimateBench-Plus//DKL Gaussian Process//data//processed_data//'\n",
    "simulations = ['ssp126', 'ssp370', 'ssp585', 'hist-GHG', 'hist-aer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'time': 251, 'longitude': 144, 'latitude': 96}) ssp126\n",
      "FrozenMappingWarningOnValuesAccess({'time': 251, 'longitude': 144, 'latitude': 96}) ssp370\n",
      "FrozenMappingWarningOnValuesAccess({'time': 251, 'longitude': 144, 'latitude': 96}) ssp585\n",
      "FrozenMappingWarningOnValuesAccess({'time': 165, 'longitude': 144, 'latitude': 96}) hist-GHG\n",
      "FrozenMappingWarningOnValuesAccess({'time': 165, 'longitude': 144, 'latitude': 96}) hist-aer\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i, simu in enumerate(simulations):\n",
    "    input_name = 'inputs_' + simu + '.nc'\n",
    "    output_name = 'outputs_' + simu + '.nc'\n",
    "    # Just load hist data in these cases 'hist-GHG' and 'hist-aer'\n",
    "    if 'hist' in simu:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_dataset(datapath + input_name)\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.open_dataset(datapath + output_name).mean(dim='member')\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400, \"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "    \n",
    "    # Concatenate with historical data in the case of scenario 'ssp126', 'ssp370' and 'ssp585'\n",
    "    else:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_mfdataset([datapath + 'inputs_historical.nc', datapath + input_name]).compute()\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.concat([xr.open_dataset(datapath + 'outputs_historical.nc').mean(dim='member'),\n",
    "                               xr.open_dataset(datapath + output_name).mean(dim='member')],\n",
    "                               dim='time').compute()\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400,\"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "\n",
    "    print(input_xr.dims, simu)\n",
    "\n",
    "    # Append to list \n",
    "    X_train.append(input_xr)\n",
    "    Y_train.append(output_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074.172303244536, 1755.690699230666)\n",
      "(0.1927369743762821, 0.18457590641432994)\n",
      "(2.5623359997066755e-12, 2.250114566783271e-11)\n",
      "(1.4947905009818064e-13, 1.0313342554838387e-12)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean/std of each variable for the whole dataset\n",
    "meanstd_inputs = {}\n",
    "len_historical = 165\n",
    "\n",
    "for var in ['CO2', 'CH4', 'SO2', 'BC']:\n",
    "    # To not take the historical data into account several time we have to slice the scenario datasets\n",
    "    # and only keep the historical data once (in the first ssp index 0 in the simus list)\n",
    "    array = np.concatenate([X_train[i][var].data for i in [0, 3, 4]] + \n",
    "                           [X_train[i][var].sel(time=slice(len_historical, None)).data for i in range(1, 3)])\n",
    "    print((array.mean(), array.std()))\n",
    "    meanstd_inputs[var] = (array.mean(), array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input data \n",
    "X_train_norm = [] \n",
    "for i, train_xr in enumerate(X_train): \n",
    "    for var in ['CO2', 'CH4', 'SO2', 'BC']: \n",
    "        var_dims = train_xr[var].dims\n",
    "        train_xr=train_xr.assign({var: (var_dims, normalize(train_xr[var].data, var, meanstd_inputs))}) \n",
    "    X_train_norm.append(train_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 4, 96, 144)\n",
      "(726, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "var_to_predict =  'tas'\n",
    "# skip_historical set to (i < 2) because of the order of the scenario and historical runs in the X_train and Y_train lists.\n",
    "# In details: ssp126 0, ssp370 1 = skip historical part of the data, ssp585 2, hist-GHG 3 and hist-aer 4 = keep the whole sequence\n",
    "X_train_all = np.concatenate([input_for_training(X_train_norm[i], skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis = 0)\n",
    "Y_train_all = np.concatenate([output_for_training(Y_train[i], var_to_predict, skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis=0)\n",
    "# add a dimension to the output data\n",
    "Y_train_all = Y_train_all[..., np.newaxis]\n",
    "\n",
    "\n",
    "X_train_all = X_train_all.reshape(726, 4, 96, 144)\n",
    "Y_train_all = Y_train_all.reshape(726, 1, 96, 144)\n",
    "print(X_train_all.shape)\n",
    "print(Y_train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Masks for the data\n",
    "Masks to get the cntext point with x% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 96, 144]) torch.Size([32, 1, 96, 144])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# get a test batch\n",
    "test_batch_X = X_train_all[0:32]\n",
    "test_batch_Y = Y_train_all[0:32]\n",
    "test_batch_X = torch.from_numpy(test_batch_X).float()\n",
    "test_batch_Y = torch.from_numpy(test_batch_Y).float()\n",
    "print(test_batch_X.shape, test_batch_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_lat_lng_mask(context_point_shape, num_context, num_extra_target):\n",
    "    aerosol_dim, lat_dim, lng_dim = context_point_shape\n",
    "    \n",
    "    # Empty mask\n",
    "    context_mask = torch.zeros((lat_dim, lng_dim))\n",
    "    target_mask = torch.zeros((lat_dim, lng_dim))\n",
    "\n",
    "    # random lat and lng\n",
    "    context_lat = np.random.randint(0, lat_dim, num_context)\n",
    "    context_lng = np.random.randint(0, lng_dim, num_context)\n",
    "    target_lat = np.random.randint(0, lat_dim, num_context + num_extra_target)\n",
    "    target_lng = np.random.randint(0, lng_dim, num_context + num_extra_target)\n",
    "    # set mask to 1\n",
    "    context_mask[context_lat, context_lng] = 1\n",
    "    target_mask[target_lat, target_lng] = 1\n",
    "    \n",
    "    return context_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_target_mask(data_size, num_context, num_extra_target, batch_size, one_mask=True):\n",
    "    aerosol_dim, lat, lng = data_size\n",
    "    batch_context_mask = torch.zeros(batch_size, lat, lng)\n",
    "    batch_target_mask = torch.zeros(batch_size, lat, lng)\n",
    "    \n",
    "    if one_mask:\n",
    "        context_mask, target_mask = get_random_lat_lng_mask((1, lat, lng), num_context, num_extra_target)\n",
    "        for i in range(batch_size):\n",
    "            batch_context_mask[i] = context_mask\n",
    "            batch_target_mask[i] = target_mask\n",
    "        \n",
    "        return batch_context_mask, batch_target_mask\n",
    "    else:\n",
    "        for i in range(batch_size):\n",
    "            batch_context_mask[i], batch_target_mask[i] = get_random_lat_lng_mask((1, lat, lng), num_context, num_extra_target)\n",
    "        \n",
    "    return batch_context_mask, batch_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 144]) torch.Size([32, 96, 144])\n"
     ]
    }
   ],
   "source": [
    "num_context_range = [2000, 8000]\n",
    "num_extra_target_range = [1000, 5000]\n",
    "\n",
    "num_context = np.random.randint(*num_context_range)\n",
    "num_extra_target = np.random.randint(*num_extra_target_range)\n",
    "\n",
    "data_size = (4, 96, 144)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "batch_context_mask, batch_target_mask = create_context_target_mask(data_size, num_context, num_extra_target, batch_size)\n",
    "print(batch_context_mask.shape, batch_target_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Sparse Representation\n",
    "\n",
    "Each datapoint will be represented with 2 tuples: (index, value) where the index in this case is the `latitude` and `longitude` and the value is the `value` of the pixel at that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_input(X, y, mask):\n",
    "    \"\"\"\n",
    "    Get sparse input data for training given the inputs and the mask\n",
    "    \"\"\"\n",
    "    # Get the shape of the data\n",
    "    Batch_size, num_aerosols, lat, lng = X.shape\n",
    "    num_outputs = y.shape[1]\n",
    "    \n",
    "    # Initialize sparse lists\n",
    "    X_sparse = []\n",
    "    y_sparse = []\n",
    "    \n",
    "    for i in range(Batch_size):\n",
    "        for j in range(lat):\n",
    "            for k in range(lng):\n",
    "                if mask[i, j, k] == 1:\n",
    "                    # Create the input list: [CO2, SO2, BC, CH4, lat, lng]\n",
    "                    X_sparse.append([X[i, 0, j, k], X[i, 1, j, k], X[i, 2, j, k], X[i, 3, j, k], j, k])\n",
    "                    # Create the output list: [tas, lat, lng]\n",
    "                    y_sparse.append([y[i, 0, j, k], j, k])\n",
    "                    \n",
    "    return X_sparse, y_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_input(X, y, mask):\n",
    "    \"\"\"\n",
    "    Get sparse input data for training given the inputs and the mask\n",
    "    \"\"\"\n",
    "    # Find the indices where the mask is 1\n",
    "    mask_indices = mask.nonzero(as_tuple=True)\n",
    "    \n",
    "    # Gather the corresponding elements from X and y\n",
    "    X_sparse = torch.cat([\n",
    "        X[mask_indices[0], 0, mask_indices[1], mask_indices[2]].unsqueeze(1),\n",
    "        X[mask_indices[0], 1, mask_indices[1], mask_indices[2]].unsqueeze(1),\n",
    "        X[mask_indices[0], 2, mask_indices[1], mask_indices[2]].unsqueeze(1),\n",
    "        X[mask_indices[0], 3, mask_indices[1], mask_indices[2]].unsqueeze(1),\n",
    "        mask_indices[1].unsqueeze(1).float(),  # Add latitude indices\n",
    "        mask_indices[2].unsqueeze(1).float()   # Add longitude indices\n",
    "    ], dim=1)\n",
    "    \n",
    "    y_sparse = torch.cat([\n",
    "        y[mask_indices[0], 0, mask_indices[1], mask_indices[2]].unsqueeze(1),\n",
    "        mask_indices[1].unsqueeze(1).float(),  # Add latitude indices\n",
    "        mask_indices[2].unsqueeze(1).float()   # Add longitude indices\n",
    "    ], dim=1)\n",
    "    \n",
    "    return X_sparse, y_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_context, y_context = get_sparse_input(test_batch_X, test_batch_Y, batch_context_mask)\n",
    "x_target, y_target = get_sparse_input(test_batch_X, test_batch_Y, batch_target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Base Model\n",
    "\n",
    "Implementation from - [Github](https://github.com/EmilienDupont/neural-processes/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Process_Pytorch.training import NeuralProcessTrainer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = list(zip(X_train_all, Y_train_all))\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 96, 144]) torch.Size([32, 1, 96, 144])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dl):\n",
    "    X, y = data\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClimateBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
