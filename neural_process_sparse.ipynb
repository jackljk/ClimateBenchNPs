{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/jack/Library/CloudStorage/GoogleDrive-limjackailjk@gmail.com/My Drive/UCSD/DSC/DSC180/ClimateBench - Plus/ClimateBench-Plus/DKL Gaussian Process/data/processed_data/'\n",
    "# datapath = 'G://My Drive//UCSD//DSC//DSC180//ClimateBench - Plus//ClimateBench-Plus//DKL Gaussian Process//data//processed_data//'\n",
    "simulations = ['ssp126', 'ssp370', 'ssp585', 'hist-GHG', 'hist-aer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp126\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp370\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp585\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-GHG\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-aer\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i, simu in enumerate(simulations):\n",
    "    input_name = 'inputs_' + simu + '.nc'\n",
    "    output_name = 'outputs_' + simu + '.nc'\n",
    "    # Just load hist data in these cases 'hist-GHG' and 'hist-aer'\n",
    "    if 'hist' in simu:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_dataset(datapath + input_name)\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.open_dataset(datapath + output_name).mean(dim='member')\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400, \"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "    \n",
    "    # Concatenate with historical data in the case of scenario 'ssp126', 'ssp370' and 'ssp585'\n",
    "    else:\n",
    "        # load inputs \n",
    "        input_xr = xr.open_mfdataset([datapath + 'inputs_historical.nc', datapath + input_name]).compute()\n",
    "            \n",
    "        # load outputs                                                             \n",
    "        output_xr = xr.concat([xr.open_dataset(datapath + 'outputs_historical.nc').mean(dim='member'),\n",
    "                               xr.open_dataset(datapath + output_name).mean(dim='member')],\n",
    "                               dim='time').compute()\n",
    "        output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400,\"pr90\": output_xr.pr90 * 86400})\\\n",
    "                             .rename({'lon':'longitude', 'lat': 'latitude'})\\\n",
    "                             .transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "\n",
    "    print(input_xr.dims, simu)\n",
    "\n",
    "    # Append to list \n",
    "    X_train.append(input_xr)\n",
    "    Y_train.append(output_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074.172303244536, 1755.690699230666)\n",
      "(0.1927369743762821, 0.18457590641432994)\n",
      "(2.5623359997066755e-12, 2.250114566783271e-11)\n",
      "(1.4947905009818064e-13, 1.0313342554838387e-12)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean/std of each variable for the whole dataset\n",
    "meanstd_inputs = {}\n",
    "len_historical = 165\n",
    "\n",
    "for var in ['CO2', 'CH4', 'SO2', 'BC']:\n",
    "    # To not take the historical data into account several time we have to slice the scenario datasets\n",
    "    # and only keep the historical data once (in the first ssp index 0 in the simus list)\n",
    "    array = np.concatenate([X_train[i][var].data for i in [0, 3, 4]] + \n",
    "                           [X_train[i][var].sel(time=slice(len_historical, None)).data for i in range(1, 3)])\n",
    "    print((array.mean(), array.std()))\n",
    "    meanstd_inputs[var] = (array.mean(), array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize input data \n",
    "X_train_norm = [] \n",
    "for i, train_xr in enumerate(X_train): \n",
    "    for var in ['CO2', 'CH4', 'SO2', 'BC']: \n",
    "        var_dims = train_xr[var].dims\n",
    "        train_xr=train_xr.assign({var: (var_dims, normalize(train_xr[var].data, var, meanstd_inputs))}) \n",
    "    X_train_norm.append(train_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 4, 96, 144)\n",
      "(726, 1, 96, 144)\n"
     ]
    }
   ],
   "source": [
    "var_to_predict =  'tas'\n",
    "# skip_historical set to (i < 2) because of the order of the scenario and historical runs in the X_train and Y_train lists.\n",
    "# In details: ssp126 0, ssp370 1 = skip historical part of the data, ssp585 2, hist-GHG 3 and hist-aer 4 = keep the whole sequence\n",
    "X_train_all = np.concatenate([input_for_training(X_train_norm[i], skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis = 0)\n",
    "Y_train_all = np.concatenate([output_for_training(Y_train[i], var_to_predict, skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis=0)\n",
    "# add a dimension to the output data\n",
    "Y_train_all = Y_train_all[..., np.newaxis]\n",
    "\n",
    "\n",
    "X_train_all = X_train_all.reshape(726, 4, 96, 144)\n",
    "Y_train_all = Y_train_all.reshape(726, 1, 96, 144)\n",
    "print(X_train_all.shape)\n",
    "print(Y_train_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Sparse Representation\n",
    "\n",
    "Each datapoint will be represented with 2 tuples: (index, value) where the index in this case is the `latitude` and `longitude` and the value is the `value` of the pixel at that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
